{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91f732e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f871b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate du^ n / dt^n for arbitrary n and use this to create a loss term\n",
    "def compute_derivative(model, t, norm_term):\n",
    "    \n",
    "    # compute derivative of outputs with respect to inputs \n",
    "    deriv_to_t = torch.autograd.functional.jacobian(model, t, create_graph=True).sum(axis=2)\n",
    "              \n",
    "    return deriv_to_t\n",
    "\n",
    "# build the loss function\n",
    "def loss(x, A, v, model): # A is matrix of coefs and v is initial conditions\n",
    "    \n",
    "    # loss results\n",
    "    loss_dict = {}\n",
    "            \n",
    "    # create the trial solution\n",
    "    model_result = lambda t: model(t)[0]\n",
    "    u = model_result(x).unsqueeze(dim = -1)\n",
    "\n",
    "    A_times_u = torch.matmul(A, u)\n",
    "    \n",
    "    # print(\"u shape: \", u.shape)\n",
    "    # print(\"A_times_u: \", A_times_u.shape)\n",
    "\n",
    "    # the line below is calling the model_outputs function which returns the solution of the network... we then sum across axis=2 to get the derivatives with respect to each u\n",
    "    du_dt = compute_derivative(model_result, x, 0)\n",
    "    \n",
    "    # print(\"du_dt shape: \", du_dt.shape)\n",
    "\n",
    "    L_t_term = du_dt + A_times_u\n",
    "    L_t = torch.matmul(L_t_term.mT, L_t_term)\n",
    "        \n",
    "    u_0 = model_result(torch.tensor([[0]], dtype=torch.float32))[0].unsqueeze(dim =-1)\n",
    "    L_0_term = u_0 - v\n",
    "    L_0 = torch.matmul(L_0_term.T, L_0_term)\n",
    "    \n",
    "    L = torch.mean(L_t) + L_0\n",
    "    \n",
    "    # normalize loss and use it to compute the normalization factor\n",
    "    # norm_term1, norm_term2 = 1 / As[0] ** 2, 1 / As[1] ** 2 ---> these are now 1 / v[0] **2 (I.C.)\n",
    "    \n",
    "    loss_dict['L_D'] = L\n",
    "    loss_dict['L_total'] = L\n",
    "\n",
    "    return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a1adb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to build the network \n",
    "class BuildNetwork(nn.Module):\n",
    "    def __init__(self, input_size, h_size1, h_size2, h_size3, output_size):\n",
    "        super(BuildNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, h_size1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.l2 = nn.Linear(h_size1, h_size2)\n",
    "        self.l3 = nn.Linear(h_size2, h_size3)\n",
    "        self.output = nn.Linear(h_size3, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        result = self.l1(x)\n",
    "        result = self.tanh(result)\n",
    "        result = self.l2(result)\n",
    "        result = self.tanh(result)\n",
    "        result = self.l3(result)\n",
    "        h = self.tanh(result)\n",
    "        result = self.output(h)\n",
    "        \n",
    "        return result, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b54931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate the model\n",
    "def run_model(iterations, x_range, A, v, hid_lay, num_equations): # A is matrix of coefs and v is initial conditions\n",
    "          \n",
    "    # build the neural net model\n",
    "    model = BuildNetwork(1, hid_lay[0], hid_lay[1], hid_lay[2], num_equations) # (1, 64, 64, 128, 2)\n",
    "    # set-up the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    # create a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "    \n",
    "    # extract the min and max range of x values to sample\n",
    "    min_x, max_x = x_range\n",
    "    \n",
    "    # create a random number generator for loss computation\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    # store loss and mse values\n",
    "    loss_history = defaultdict(list)\n",
    "    MSEs = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "#     curr_A_indx = -1 NOTE: commented out until we do transfer learning\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        print(\"Iteration: \", i)\n",
    "        \n",
    "        if i % 2000 == 0: \n",
    "#             curr_A_indx += 1 NOTE: commented out until we do transfer learning\n",
    "#             A = A_vals[curr_A_indx] NOTE: commented out until we do transfer learning\n",
    "#             print(f\"Iteration {i} completed\")\n",
    "            pass\n",
    "        \n",
    "        # every batch, randomly sample from min and max range\n",
    "        x = torch.arange(min_x, max_x, 0.001, requires_grad=True)\n",
    "        x = x[rng.integers(low=0, high=len(x), size=500)]\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # forward: compute loss\n",
    "        curr_loss = loss(x, A, v, model)\n",
    "        \n",
    "        # store individual loss terms for plotting\n",
    "        loss_history['LD_losses'].append(curr_loss['L_D'].item())\n",
    "        loss_history['Ltotal_losses'].append(curr_loss['L_total'].item())\n",
    "            \n",
    "        # backward: backpropagation\n",
    "        curr_loss['L_total'].backward()\n",
    "        \n",
    "        # update weights and reset the gradients\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # learning rate update\n",
    "        # scheduler.step()\n",
    "        \n",
    "        # compute the mse\n",
    "        with torch.no_grad():\n",
    "            # TODO: true solutions will have to be passed in and generalize it to n equations\n",
    "            e_term1, e_term2 = np.exp(-1.1 * x), np.exp(2.2 * x)\n",
    "            true_sol1 = (0.5 * A[0] * e_term1 * (e_term2 + 1) - (0.5 * A[1] * e_term1 * (e_term2 - 1))).numpy()\n",
    "            network_sol1 = model(x)[0][:, 0].unsqueeze(dim=1).numpy()\n",
    "            true_sol2 = (0.5 * A[1] * e_term1 * (e_term2 + 1) - (0.5 * A[0] * e_term1 * (e_term2 - 1))).numpy()\n",
    "            network_sol2 = model(x)[0][:, 1].unsqueeze(dim=1).numpy()\n",
    "            \n",
    "        current_mse = np.mean((true_sol1 - network_sol1) ** 2) + np.mean((true_sol2 - network_sol2) ** 2)\n",
    "        MSEs.append(current_mse)\n",
    "   \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Model Training Complete in{total_time: .3f} seconds\")\n",
    "    \n",
    "    return loss_history, model, total_time, MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c934eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to plot the neural network vs exact solution\n",
    "# def plot_solution(min_x, max_x, As, lambda_vals, trained_model, axis):\n",
    "    \n",
    "#     u1_t = lambda t: trained_model(t)[0][:, 0]\n",
    "#     u2_t = lambda t: trained_model(t)[0][:, 1]\n",
    "    \n",
    "#     # plot the solution accuracies\n",
    "#     xx = np.linspace(min_x, max_x, 200)[:, None]\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         yy1 = u1_t(torch.Tensor(xx)).numpy()\n",
    "#         yy2 = u2_t(torch.Tensor(xx)).numpy()\n",
    "\n",
    "#     # TODO: extract this plotting code out of here and make it something that is passed in\n",
    "#     # TODO: make the plotting generalize to n equations (i.e. labels will have to be passed in)\n",
    "#     e_term1, e_term2 = np.exp(-1.1 * xx), np.exp(2.2 * xx)\n",
    "#     yt1 = (0.5 * As[0] * e_term1 * (e_term2 + 1) - (0.5 * As[1] * e_term1 * (e_term2 - 1)))\n",
    "#     yt2 = (0.5 * As[1] * e_term1 * (e_term2 + 1) - (0.5 * As[0] * e_term1 * (e_term2 - 1)))\n",
    "\n",
    "#     axis.plot(xx, yt1, label='Actual Solution $u_1(t)$')\n",
    "#     axis.plot(xx, yt2, label='Actual Solution $u_2(t)$')\n",
    "#     axis.plot(xx, yy1, '--', label='Neural Network Solution $u_1(t)$')\n",
    "#     axis.plot(xx, yy2, '--', label='Neural Network Solution $u_2(t)$')\n",
    "#     axis.set_title(\"u(t) vs t for Actual and Approximation\")\n",
    "#     axis.set_xlabel('$t$')\n",
    "#     axis.set_ylabel('$u(t)$')\n",
    "#     axis.legend(loc='best')\n",
    "    \n",
    "# # function to plot the overall loss of the network solution\n",
    "# def plot_total_loss(iterations, train_losses, axis, loss_label):\n",
    "#     axis.plot(range(iterations), train_losses, label=loss_label)\n",
    "#     axis.set_yscale(\"log\")\n",
    "#     axis.set_title(\"Total Loss vs Iterations\")\n",
    "#     axis.set_xlabel('Iterations')\n",
    "#     axis.set_ylabel('Loss')\n",
    "#     axis.legend(loc='best')\n",
    "    \n",
    "# # function to plot the MSEs\n",
    "# def plot_mse(iterations, mses, axis):\n",
    "#     axis.plot(range(iterations), mses, label='MSE')\n",
    "#     axis.set_yscale(\"log\")\n",
    "#     axis.set_title(\"MSE vs Iterations\")\n",
    "#     axis.set_xlabel('Iterations')\n",
    "#     axis.set_ylabel('MSE')\n",
    "#     axis.legend(loc='best')\n",
    "    \n",
    "# # wrapper function to plot the solution and the overall loss of the network solution\n",
    "# def plot_loss_mse_and_solution(min_x, max_x, As, lambda_vals, iterations, \n",
    "#                                trained_model, train_losses, loss_label, mses):\n",
    "#     fig, axs = plt.subplots(1, 3,  tight_layout=True, figsize=(24, 8))\n",
    "    \n",
    "#     plot_total_loss(iterations=iterations, train_losses=train_losses, \n",
    "#                     axis=axs[0], loss_label=loss_label)\n",
    "#     plot_solution(min_x=min_x, max_x=max_x, As=As, lambda_vals=lambda_vals, \n",
    "#                   trained_model=trained_model, axis=axs[1])\n",
    "#     plot_mse(iterations=iterations, mses=mses, axis=axs[2])\n",
    "    \n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c7b6ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Model Training Complete in 13.223 seconds\n"
     ]
    }
   ],
   "source": [
    "# set-up differential equation parameters for network\n",
    "x_range = [0, 2]\n",
    "A = torch.tensor([[0, 1.1], [1.1, 0]])\n",
    "v = torch.tensor([[0.75], [0.25]])\n",
    "hid_lay = [64, 64, 128]\n",
    "num_equations = 2\n",
    "iterations = 10\n",
    "\n",
    "# run model which has a loss of the MSE (i.e. no higher order terms)\n",
    "loss_history_w1, trained_model_w1, time_w1, MSE_w1  = run_model(iterations=iterations, x_range=x_range, A=A, v=v, hid_lay=hid_lay, num_equations=num_equations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration:  0\n",
    "# Iteration:  1\n",
    "# Iteration:  2\n",
    "# Iteration:  3\n",
    "# Iteration:  4\n",
    "# Iteration:  5\n",
    "# Iteration:  6\n",
    "# Iteration:  7\n",
    "# Iteration:  8\n",
    "# Iteration:  9\n",
    "# Model Training Complete in 13.223 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9761d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(0.0522 - 0.2502)\n",
    "# print(-0.0353 + 0.0705)\n",
    "# a = torch.ones([500, 2, 1])\n",
    "# print(torch.matmul(a.mT, a).shape)\n",
    "# print(torch.matmul(a, a.mT).shape)\n",
    "# - 0.1804 -0.25\n",
    "\n",
    "tensor([[0.5288]], grad_fn=<AddBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb76ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.tensor([[1, 2], [3, 4]])\n",
    "# b = torch.tensor([[[10], [20]], [[30], [40]], [[50], [60]]])\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "\n",
    "# print(torch.matmul(a, b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e20fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_data = [[1, 2], [3, 4]]\n",
    "# M = torch.tensor(M_data)\n",
    "# print(M)\n",
    "\n",
    "# torch.matmul(M, M)\n",
    "A = torch.tensor([[0, 1.1], [1.1, 0]])\n",
    "print(A)\n",
    "v = torch.tensor([0.75, 0.25])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [0.75, 0.25]\n",
    "xx = torch.Tensor(np.linspace(0, 2, 200)[:, None])\n",
    "u_1 = trained_model_w1(xx)[0][:, 0].detach().numpy()\n",
    "u_2 = trained_model_w1(xx)[0][:, 1].detach().numpy()\n",
    "\n",
    "e_term1, e_term2 = np.exp(-1.1 * xx), np.exp(2.2 * xx)\n",
    "\n",
    "yt1 = (0.5 * A[0] * e_term1 * (e_term2 + 1) - (0.5 * A[1] * e_term1 * (e_term2 - 1))).numpy()\n",
    "yt2 = (0.5 * A[1] * e_term1 * (e_term2 + 1) - (0.5 * A[0] * e_term1 * (e_term2 - 1))).numpy()\n",
    "\n",
    "plt.scatter(xx, u_1, label='u1')\n",
    "plt.scatter(xx, yt1, label='sol u1', s=5)\n",
    "plt.scatter(xx, u_2, label='u2')\n",
    "plt.scatter(xx, yt2, label='sol u2', s=5)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88962d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overall loss and network solution for model which has a loss of the MSE (i.e. no higher order terms)\n",
    "print(\"\\n Model 1: Loss = L_D\")\n",
    "plot_loss_mse_and_solution(min_x=min_x, max_x=max_x, As=A_vals[3], lambda_vals=lambda_vals, \n",
    "                       iterations=iterations, trained_model=trained_model_w1, \n",
    "                       train_losses=loss_history_w1['Ltotal_losses'], \n",
    "                       loss_label='Total Loss ($L_D$)', mses=MSE_w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2bdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a t to take derivative with respect to \n",
    "# rng = np.random.default_rng()\n",
    "# t_eval = torch.arange(0, 2, 0.001, requires_grad=True)\n",
    "# t_eval = t_eval[rng.integers(low=0, high=2000, size=500)]\n",
    "# t_eval = t_eval.reshape(-1, 1)\n",
    "\n",
    "# # forward pass to extract H \n",
    "# output, H = trained_model_w1(t_eval) \n",
    "\n",
    "# print(\"H shape before column of 1s: \", H.shape)\n",
    "# H = torch.cat((torch.ones(len(t_eval), 1), H), 1)\n",
    "# print(\"H shape after column of 1s: \", H.shape)\n",
    "\n",
    "# # compute derivative w/r/t H and multiply it by its transpose\n",
    "# dH_dt = torch.autograd.functional.jacobian(trained_model_w1 ,t_eval, create_graph=True)[1].sum(axis=2)\n",
    "\n",
    "# print(\"dh_dt before adjusting: \", dH_dt.shape)\n",
    "# dH_dt = torch.cat((torch.zeros(len(t_eval), 1, 1), dH_dt), 1)\n",
    "# print(\"dh_dt after adjusting: \", dH_dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89234f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute dH_dt times dH_dt transpose \n",
    "# dH_dt_times_dH_dt_T = torch.matmul(dH_dt, dH_dt.mT)\n",
    "# print(\"dH_dt * dH_dt_T: \", dH_dt_times_dH_dt_T.shape)\n",
    "\n",
    "# # compute dH_dt times H transpose \n",
    "# new_H = torch.unsqueeze(H, dim=-1)\n",
    "# dH_dt_times_H_T = torch.matmul(dH_dt, new_H.mT)\n",
    "# print(\"dH_dt * H_T: \", dH_dt_times_H_T.shape)\n",
    "\n",
    "# # compute H times dH_dt transpose\n",
    "# H_times_dH_dt_T = torch.matmul(new_H, dH_dt.mT)\n",
    "\n",
    "# # compute H times H transpose\n",
    "# H_times_H_T = torch.matmul(new_H, new_H.mT)\n",
    "# print(\"H * H_T: \", H_times_H_T.shape)\n",
    "\n",
    "# # compute the sum of matrices across time\n",
    "# # w_sum_term = dH_dt_times_dH_dt_T + 2 * lambda_val * dH_dt_times_H_T + lambda_val ** 2 * H_times_H_T\n",
    "# w_sum_term = dH_dt_times_dH_dt_T + lambda_val * dH_dt_times_H_T + lambda_val * H_times_dH_dt_T + lambda_val ** 2 * H_times_H_T \n",
    "\n",
    "# print(\"W_sum shape before summing across time: \", w_sum_term.shape)\n",
    "# w_sum_term = w_sum_term.sum(axis=0)\n",
    "# w_sum_term = w_sum_term / len(t_eval)\n",
    "# print(\"W_sum shape after summing across time: \", w_sum_term.shape)\n",
    "\n",
    "# # compute H_0 and multiply it by its transpose \n",
    "# output_0, H_0 = trained_model_w1(torch.tensor([[0.]], dtype=torch.float32, requires_grad=True))\n",
    "# print(\"H_0 shape before appending 1: \", H_0.shape)\n",
    "# H_0 = torch.cat((torch.ones(1, 1), H_0), 1)\n",
    "# print(\"H_0 shape after appending 1: \", H_0.shape)\n",
    "\n",
    "# H_0_T_times_H_0 = torch.matmul(H_0.T, H_0)\n",
    "# print(\"H_0_T * H_0 shape: \", H_0_T_times_H_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the M matrix and find its inverse \n",
    "# M = w_sum_term + H_0_T_times_H_0\n",
    "# M_inverse = torch.linalg.pinv(M)\n",
    "\n",
    "# # compute W_out\n",
    "# w_out = torch.matmul(M_inverse, H_0.T) * 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52eede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranfer_learned = [torch.matmul(w_out.T, new_H[i]).item() for i in range(500)]\n",
    "# times = list(t_eval.detach().numpy().squeeze())\n",
    "\n",
    "# true = [0.75 * np.exp(-lambda_val * time) for time in times]\n",
    "# plt.scatter(times, tranfer_learned, label = 'Transfer Learning')\n",
    "# plt.scatter(times, true, label='Analytical', s=10)\n",
    "# plt.xlabel(\"t\")\n",
    "# plt.ylabel('y')\n",
    "# plt.title(\"Transfer Learning vs Analytical Solution\")\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f04a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_one_shot_time = time.time()\n",
    "# rng = np.random.default_rng(12345)\n",
    "# for i in range(10000):\n",
    "#     random_u_0 = rng.random()\n",
    "#     w_out = torch.matmul(M_inverse, H_0.T) * random_u_0\n",
    "    \n",
    "# end_one_shot_time = time.time() \n",
    "# total_one_shot = end_one_shot_time - start_one_shot_time\n",
    "# print(f\"Total Time to evaluate w_out for 10000 different initial conditions: {total_one_shot: .2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7786342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "# 1. make code work with 2 linear ODEs\n",
    "\n",
    "# 2. make the code generalize to a system of n ODEs\n",
    "\n",
    "# 3. compute the individual terms needed in the w_out calculation for a system\n",
    "\n",
    "# 4. see how formula works and what can be changed while still doing it in \"one shot\"\n",
    "\n",
    "# 5. Are the higher order terms, L_D and L_DD, etc.. still relevant?\n",
    "\n",
    "# 6. see which parts of the formula are dependent on time and ask Pavlos how that works\n",
    "    # (changing W_o means that only u_o will change eventually?)\n",
    "\n",
    "# 7. try using a different diff. equation\n",
    "\n",
    "# 9. make loss function proper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0698eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals = (torch.matmul(w_out.T, dH_dt[:]) + lambda_val * torch.matmul(w_out.T, new_H[:])) ** 2\n",
    "# residuals = residuals.reshape(500, 1).detach().numpy()\n",
    "\n",
    "# print(f\"Mean of residuals: {residuals.mean()}\")\n",
    "# initial_cond_residuals = (torch.matmul(w_out.T, H_0.T) - 0.75) ** 2\n",
    "# print(f\"Initial Condition Residual: {initial_cond_residuals.item()}\")\n",
    "\n",
    "# plt.plot(residuals);\n",
    "# plt.xlabel(\"t\")\n",
    "# plt.ylabel(\"Residual\")\n",
    "# plt.title(\"Plot of Residuals\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Difference Between Analytical and Tranfer Learned for t ={times[0]: .3f}: {(0.75 * np.exp(-lambda_val * times[0]) - torch.matmul(w_out.T, H[0])).item(): .5f}\")\n",
    "# print(f\"Difference Between Analytical and Tranfer Learned for t ={times[100]: .3f}: {(0.75 * np.exp(-lambda_val * times[100]) - torch.matmul(w_out.T, H[100])).item(): .5f}\")\n",
    "# print(f\"Difference Between Analytical and Tranfer Learned for t ={times[200]: .3f}: {(0.75 * np.exp(-lambda_val * times[200]) - torch.matmul(w_out.T, H[200])).item(): .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# H_test = H.detach()\n",
    "# gm_H = torch.matmul(H_test.T, H_test)\n",
    "# pca = PCA(n_components=5)\n",
    "# pca.fit(gm_H)\n",
    "# print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a600e",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7898bc",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "- $u_t = lambda$ $t: A + t * model(t)$ is outdated, so we use $ u_t = lambda$ $t: A + (1 - exp(-t)) * model(t)$\n",
    "\n",
    "- We are solving $\\frac{du}{dt} + \\lambda * u = 0$ where $f = -\\lambda * u$\n",
    "\n",
    "- Old Model Code: model = nn.Sequential(nn.Linear(1, 64), nn.Tanh(), nn.Linear(64, 64), nn.Tanh(), nn.Linear(64,1))\n",
    "\n",
    "- Old solution parametrization: u_t = lambda t: A + (1 - torch.exp(-t)) * model(t)\n",
    "\n",
    "- Original normalization term: norm_term = 1 / u_t(torch.tensor([[0]], dtype=torch.float32)) ** 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
