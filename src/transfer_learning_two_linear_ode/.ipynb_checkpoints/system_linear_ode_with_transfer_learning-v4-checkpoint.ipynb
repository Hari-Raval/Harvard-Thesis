{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f732e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HariRaval/miniconda3/envs/thesis/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60f378",
   "metadata": {},
   "source": [
    "# Compute Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f871b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate du^n / dt^n for arbitrary n and use this to form loss\n",
    "def compute_derivative(u, t, norm_term, num_u_vectors):\n",
    "    \n",
    "    # compute derivative of outputs with respect to inputs \n",
    "    derivs_list = []\n",
    "    for i in range(num_u_vectors):\n",
    "        # individually compute du/dt for each u and combine them all together afterwards\n",
    "        du_dt = torch.autograd.grad(u[:, i, :], t, grad_outputs=torch.ones_like(u[:, i, :]), create_graph=True)[0]\n",
    "        derivs_list.append(du_dt)\n",
    "\n",
    "    deriv_to_t = torch.stack(derivs_list, dim=1) \n",
    "    \n",
    "    return deriv_to_t\n",
    "\n",
    "# build the loss function\n",
    "def loss(x, A_list, v_list, model):\n",
    "    \n",
    "    # dictionary to store loss results for plotting\n",
    "    loss_dict = {} \n",
    "    # current loss\n",
    "    L = 0\n",
    "    \n",
    "    # create the trial solution\n",
    "    model_result = lambda t: model(t)[0]\n",
    "    u_results = model_result(x)\n",
    "    \n",
    "    # determine the number of u vectors\n",
    "    num_u_vectors = u_results['head 1'].shape[1]\n",
    "    \n",
    "    # loss will be the sum of the terms from the \"multi-head\" model, hence we iterate over each head's outputs\n",
    "    for i, head_i in enumerate(u_results.keys()):\n",
    "        # extract the u for the current \"head\", corresponding to one of the initial conditions\n",
    "        u = u_results[head_i].unsqueeze(dim=-1)\n",
    "        \n",
    "        print(\"u shape: \", u.shape)\n",
    "        print(\"A shape: \", A_list[i].shape)\n",
    "        \n",
    "        # compute A * u\n",
    "        A_times_u = torch.matmul(A_list[i], u)\n",
    "            \n",
    "        # compute du/dt (Note: each u is computed separately to avoid torch.autograd.grad() summing them together)\n",
    "        du_dt = compute_derivative(u, x, 0, num_u_vectors)\n",
    "\n",
    "        print(\"A_times_u shape: \", A_times_u.shape)\n",
    "        print(\"du_dt shape: \", du_dt.shape)\n",
    "        \n",
    "        # compute the L_T term\n",
    "        L_t_term = du_dt + A_times_u\n",
    "        \n",
    "        print(\"L_t_term shape: \", L_t_term.shape)\n",
    "        L_t = torch.matmul(L_t_term.mT, L_t_term)\n",
    "        print(\"L_t shape: \", L_t.shape)\n",
    "\n",
    "        # compute the L_0 term\n",
    "        u_0 = model_result(torch.tensor([[0]], dtype=torch.float32))[head_i][0].unsqueeze(dim=-1)\n",
    "        print(\"u_0 shape: \", u_0.shape)\n",
    "        print(\"v shape: \", v_list[i].shape)\n",
    "        L_0_term = u_0 - v_list[i]\n",
    "        L_0 = torch.matmul(L_0_term.T, L_0_term)\n",
    "    \n",
    "        # compute the overall loss \n",
    "        L += (torch.mean(L_t) + L_0)\n",
    "    \n",
    "    # normalize loss and use it to compute the normalization factor\n",
    "    # norm_term1, norm_term2 = 1 / As[0] ** 2, 1 / As[1] ** 2 ---> these are now 1 / v[0] **2 (I.C.)\n",
    "    \n",
    "    loss_dict['L_D'] = L\n",
    "    loss_dict['L_total'] = L\n",
    "\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Iteration 1\n",
    "# u shape:  torch.Size([500, 2, 1])\n",
    "# A shape:  torch.Size([2, 2])\n",
    "# A_times_u shape:  torch.Size([500, 2, 1])\n",
    "# du_dt shape:  torch.Size([500, 2, 1])\n",
    "# L_t_term shape:  torch.Size([500, 2, 1])\n",
    "# L_t shape:  torch.Size([500, 1, 1])\n",
    "# u_0 shape:  torch.Size([2, 1])\n",
    "# v shape:  torch.Size([2, 1])\n",
    "\n",
    "# u shape:  torch.Size([500, 2, 1])\n",
    "# A shape:  torch.Size([2, 2])\n",
    "# A_times_u shape:  torch.Size([500, 2, 1])\n",
    "# du_dt shape:  torch.Size([500, 2, 1])\n",
    "# L_t_term shape:  torch.Size([500, 2, 1])\n",
    "# L_t shape:  torch.Size([500, 1, 1])\n",
    "# u_0 shape:  torch.Size([2, 1])\n",
    "# v shape:  torch.Size([2, 1])\n",
    "\n",
    "# u shape:  torch.Size([500, 2, 1])\n",
    "# A shape:  torch.Size([2, 2])\n",
    "# A_times_u shape:  torch.Size([500, 2, 1])\n",
    "# du_dt shape:  torch.Size([500, 2, 1])\n",
    "# L_t_term shape:  torch.Size([500, 2, 1])\n",
    "# L_t shape:  torch.Size([500, 1, 1])\n",
    "# u_0 shape:  torch.Size([2, 1])\n",
    "# v shape:  torch.Size([2, 1])\n",
    "\n",
    "# u shape:  torch.Size([500, 2, 1])\n",
    "# A shape:  torch.Size([2, 2])\n",
    "# A_times_u shape:  torch.Size([500, 2, 1])\n",
    "# du_dt shape:  torch.Size([500, 2, 1])\n",
    "# L_t_term shape:  torch.Size([500, 2, 1])\n",
    "# L_t shape:  torch.Size([500, 1, 1])\n",
    "# u_0 shape:  torch.Size([2, 1])\n",
    "# v shape:  torch.Size([2, 1])\n",
    "\n",
    "# Model Training Complete in 0.060 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8d6af",
   "metadata": {},
   "source": [
    "# Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a1adb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to build the network \n",
    "class BuildNetwork(nn.Module):\n",
    "    def __init__(self, input_size, h_size1, h_size2, h_size3, output_size, n_heads):\n",
    "        super(BuildNetwork, self).__init__()\n",
    "        # store the number of \"heads\" to use in the model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # build the layers to use for the forward pass\n",
    "        self.l1 = nn.Linear(input_size, h_size1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.l2 = nn.Linear(h_size1, h_size2)\n",
    "        self.l3 = nn.Linear(h_size2, h_size3)\n",
    "        \n",
    "        # build n_heads output layers, each corresponding to different conditions during training\n",
    "        self.multi_head_output= nn.ModuleList([nn.Linear(h_size3, output_size)])\n",
    "        self.multi_head_output.extend([nn.Linear(h_size3, output_size) for i in range(n_heads-1)])\n",
    "         \n",
    "    def forward(self, x):\n",
    "        # dictionary to store the output for each \"head\" in the model\n",
    "        u_results = {}\n",
    "        \n",
    "        # all \"heads\" have the same pass through the hidden laers\n",
    "        result = self.l1(x)\n",
    "        result = self.tanh(result)\n",
    "        result = self.l2(result)\n",
    "        result = self.tanh(result)\n",
    "        result = self.l3(result)\n",
    "        h = self.tanh(result)\n",
    "        \n",
    "        # apply the corresponding output layer to each \"head\"\n",
    "        for i in range(self.n_heads):\n",
    "            result_i = self.multi_head_output[i](h)\n",
    "            u_results[f\"head {i + 1}\"] = result_i\n",
    "            \n",
    "        return u_results, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add8fa9",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b54931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and evaluate the model\n",
    "def run_model(iterations, x_range, A_list, v_list, hid_lay, num_equations, num_heads, true_functs, head_to_track):\n",
    "        \n",
    "    assert num_equations > 0, 'num_equations must be >= 1'\n",
    "    assert len(true_functs) == num_equations, 'num_equations must equal the length of true_functs'\n",
    "    assert len(v_list) == num_heads, 'num_heads must equal the length of v_list'\n",
    "    assert len(A_list) == num_heads, 'num_heads must equal the length of A_list'\n",
    "    \n",
    "    # build the neural net model\n",
    "    model = BuildNetwork(1, hid_lay[0], hid_lay[1], hid_lay[2], num_equations, num_heads)\n",
    "    # set-up the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    # create a learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "    \n",
    "    # extract the min and max range of x values to sample\n",
    "    min_x, max_x = x_range\n",
    "    \n",
    "    # index of the head being tracked for MSE\n",
    "    head_idx = int(head_to_track.split()[-1]) - 1\n",
    "    \n",
    "    # create a random number generator for loss computation\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    # store loss and mse values\n",
    "    loss_history = defaultdict(list)\n",
    "    MSEs = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # training loop\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Running Iteration {i + 1}\")\n",
    "        \n",
    "        # every batch, randomly sample from min and max range\n",
    "        x = torch.arange(min_x, max_x, 0.001, requires_grad=True)\n",
    "        x = x[rng.integers(low=0, high=len(x), size=500)] # TODO: change size back to 500\n",
    "        x = x.reshape(-1, 1)\n",
    "\n",
    "        # forward: compute loss\n",
    "        curr_loss = loss(x, A_list, v_list, model)\n",
    "        \n",
    "        # store individual loss terms for plotting\n",
    "        loss_history['LD_losses'].append(curr_loss['L_D'].item())\n",
    "        loss_history['Ltotal_losses'].append(curr_loss['L_total'].item())\n",
    "            \n",
    "        # backward: backpropagation\n",
    "        curr_loss['L_total'].backward()\n",
    "        \n",
    "        # update weights and reset the gradients\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "        \n",
    "        # learning rate update\n",
    "        # scheduler.step()\n",
    "        \n",
    "        # compute the mse for the head that is being monitored ('head_to_track')\n",
    "        with torch.no_grad():\n",
    "            current_mse = 0\n",
    "            for j in range(num_equations):\n",
    "                network_sol_j = model(x)[0][head_to_track][:, j].unsqueeze(dim=1).numpy()\n",
    "                true_sol_j = true_functs[j](x, v_list[head_idx])\n",
    "                current_mse += np.mean((true_sol_j - network_sol_j) ** 2)\n",
    "            MSEs.append(current_mse)\n",
    "               \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Model Training Complete in{total_time: .3f} seconds\")\n",
    "    \n",
    "    return loss_history, model, total_time, MSEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653d9cd",
   "metadata": {},
   "source": [
    "# Plot Solution, Loss, and MSE Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c934eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot the neural network vs exact solution\n",
    "def plot_solution(x_range, true_functs, trained_model, v_list, axis, head_to_track):\n",
    "    \n",
    "    # function to extract the model results\n",
    "    model_result = lambda t: trained_model(t)[0]\n",
    "    \n",
    "    # x values to predict on\n",
    "    min_x, max_x = x_range\n",
    "    xx = np.linspace(min_x, max_x, 200)[:, None]\n",
    "\n",
    "    # find the model results\n",
    "    u = model_result(torch.Tensor(xx))[head_to_track] \n",
    "    # determine the number of curves to plot\n",
    "    num_curves = u.shape[1]\n",
    "    # store the true solutions and network solutions\n",
    "    yys, yts = [], []\n",
    "\n",
    "    # save the network solutions in a list for plotting \n",
    "    with torch.no_grad():\n",
    "        head_idx = int(head_to_track.split()[-1]) - 1\n",
    "        for i in range(num_curves):\n",
    "            yys.append(u[:, i].numpy())\n",
    "            yts.append(true_functs[i](xx, v_list[head_idx]))\n",
    "                    \n",
    "    # plot the network solutions\n",
    "    for i in range(num_curves):\n",
    "        axis.plot(xx, yys[i], 'x', label=f'Network Solution $u_{i+1}(t)$ ({head_to_track})',\n",
    "                  linewidth=3.5)\n",
    "        \n",
    "    # plot the true solutions\n",
    "    for i in range(num_curves):\n",
    "         axis.plot(xx, yts[i], label=f'Actual Solution $u_{i+1}(t)$', linewidth=2.5)\n",
    "\n",
    "    axis.set_title(\"u(t) vs t for Actual and Approximation Solutions\")\n",
    "    axis.set_xlabel('$t$')\n",
    "    axis.set_ylabel('$u(t)$')\n",
    "    axis.legend(loc='best')\n",
    "    \n",
    "# function to plot the overall loss of the network solution\n",
    "def plot_total_loss(iterations, train_losses, axis, loss_label):\n",
    "    axis.plot(range(iterations), train_losses, label=loss_label)\n",
    "    axis.set_yscale(\"log\")\n",
    "    axis.set_title(\"Total Loss vs Iterations\")\n",
    "    axis.set_xlabel('Iterations')\n",
    "    axis.set_ylabel('Loss')\n",
    "    axis.legend(loc='best')\n",
    "    \n",
    "# function to plot the MSEs\n",
    "def plot_mse(iterations, mses, axis, head_to_track):\n",
    "    axis.plot(range(iterations), mses, label=f'MSE ({head_to_track})')\n",
    "    axis.set_yscale(\"log\")\n",
    "    axis.set_title(\"MSE vs Iterations\")\n",
    "    axis.set_xlabel('Iterations')\n",
    "    axis.set_ylabel('MSE')\n",
    "    axis.legend(loc='best')\n",
    "    \n",
    "# wrapper function to plot the solution and the overall loss & MSE of the network solution\n",
    "def plot_loss_mse_and_solution(x_range, true_functs, iterations, trained_model, v_list, \n",
    "                               train_losses, loss_label, mses, head_to_track):\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3,  tight_layout=True, figsize=(24, 8))\n",
    "    \n",
    "    plot_total_loss(iterations=iterations, train_losses=train_losses, \n",
    "                    axis=axs[0], loss_label=loss_label)\n",
    "    \n",
    "    plot_solution(x_range=x_range, true_functs=true_functs, \n",
    "                  trained_model=trained_model, v_list=v_list,\n",
    "                  axis=axs[1], head_to_track=head_to_track)\n",
    "    \n",
    "    plot_mse(iterations=iterations, mses=mses, axis=axs[2], \n",
    "             head_to_track=head_to_track)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e919b",
   "metadata": {},
   "source": [
    "# Find Output Layer Weights with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd9b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_H_and_dH_dt(min_x, max_x, trained_model, num_equations, hid_lay):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # generate a set of times to evaluate with\n",
    "    rng = np.random.default_rng()\n",
    "    t_eval = torch.arange(min_x, max_x, 0.001, requires_grad=True)\n",
    "    t_eval = t_eval[rng.integers(low=0, high=len(t_eval), size=500)]\n",
    "    t_eval = t_eval.reshape(-1, 1)\n",
    "    \n",
    "    # forward pass with t_eval to extract H \n",
    "    output, H = trained_model(t_eval)\n",
    "    # reshape \"H\" to batch_size X num_equations X d // num_equations\n",
    "    H = H.reshape(-1, num_equations, hid_lay[-1] // num_equations)\n",
    "    H = torch.cat((torch.ones(len(t_eval), num_equations, 1), H), 2)\n",
    "    \n",
    "    # forward pass with t = 0 to extract H_0\n",
    "    output_0, H_0 = trained_model(torch.tensor([[0.]], dtype=torch.float32, requires_grad=True))\n",
    "    # reshape \"H_0\" to batch_size X num_equations X d // num_equations\n",
    "    H_0 = H_0.reshape(-1, num_equations, hid_lay[-1] // num_equations)\n",
    "    H_0 = torch.cat((torch.ones(1, num_equations, 1), H_0), 2).squeeze()\n",
    "    H_0 = H_0.unsqueeze(dim=0) if num_equations == 1 else H_0\n",
    "\n",
    "    # compute dH_dt (note: time intensive due to large jacobian calculation)\n",
    "    trained_model_H_only = lambda x: trained_model(x)[1]    \n",
    "    dH_dt = torch.autograd.functional.jacobian(trained_model_H_only, t_eval, create_graph=False).sum(axis=2) \n",
    "    dH_dt_new = dH_dt.reshape(-1, num_equations, hid_lay[-1] // num_equations)\n",
    "    dH_dt_new = torch.cat((torch.zeros(len(t_eval), num_equations, 1), dH_dt_new), 2)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Time to compute H and dH_dt: {total_time: .3f} seconds\")\n",
    "\n",
    "    return H, H_0, dH_dt_new, t_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "601aa334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytically_compute_weights(dH_dt, H, H_0, t_eval, v, A):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # compute dH_dt * dH_dt.T\n",
    "    dH_dt_times_dH_dt_T = torch.matmul(dH_dt.mT, dH_dt)\n",
    "\n",
    "    # compute dH_dt * A * H\n",
    "    dH_dt_times_A_times_H = torch.matmul(torch.matmul(dH_dt.mT, A), H)\n",
    "\n",
    "    # compute H.T * A.T * dH_dt\n",
    "    H_times_A_T_times_dH_dt = torch.matmul(torch.matmul(H.mT, A.T), dH_dt)\n",
    "\n",
    "    # compute H.T * A.T * A * H\n",
    "    H_T_times_A_T_times_A_times_H = torch.matmul(torch.matmul(torch.matmul(H.mT, A.T), A), H)\n",
    "\n",
    "    # compute the \"summation portion\" of the M matrix \n",
    "    M_sum_terms = dH_dt_times_dH_dt_T + dH_dt_times_A_times_H + H_times_A_T_times_dH_dt + H_T_times_A_T_times_A_times_H\n",
    "    M_sum_terms = M_sum_terms.sum(axis=0)\n",
    "    M_sum_terms = M_sum_terms / len(t_eval)\n",
    "\n",
    "    # compute H_0.T * H_0\n",
    "    H_0_T_times_H_0 = torch.matmul(H_0.mT, H_0)\n",
    "\n",
    "    # compute the \"M\" matrix and invert it\n",
    "    M = M_sum_terms + H_0_T_times_H_0\n",
    "    M_inv = torch.linalg.pinv(M)\n",
    "\n",
    "    # compute the output weights by W_out = M ^ -1 * H_0 * u_0\n",
    "    W_out = torch.matmul(torch.matmul(M_inv, H_0.T), v)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Time to compute weights (given H and dH_dt): {total_time: .3f} seconds\")\n",
    "\n",
    "    return M_inv, W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b523b",
   "metadata": {},
   "source": [
    "# Plot Transfer Learned and Analytical Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a95b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transfer_learned_and_analytical(H, W_out, t_eval, v, num_equations, true_funct):\n",
    "    \n",
    "    # compute the transfer learned solution\n",
    "    u_transfer = torch.matmul(H, W_out)\n",
    "\n",
    "    # plot the true solutions\n",
    "    for i in range(num_equations):\n",
    "        plt.scatter(t_eval.detach().numpy(), true_funct[i](t_eval.detach().numpy(), v), \n",
    "                    label= f'True $U_{i+1}$');\n",
    "    \n",
    "    # plot the transfer learned solutions\n",
    "    for i in range(num_equations):\n",
    "        plt.scatter(t_eval.detach().numpy(), u_transfer[:, i, :].detach().numpy(), \n",
    "                    label=f'Transfer Learned $U_{i+1}$', s=2);\n",
    "    \n",
    "    plt.xlabel(\"t\")\n",
    "    plt.ylabel(\"$u(t)$\")\n",
    "    plt.title(\"Transfer Learned vs Analytical Solution\")\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbadee3",
   "metadata": {},
   "source": [
    "# Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e3242",
   "metadata": {},
   "source": [
    "### Step 1: Choose Network Parameters and Set-up Differential Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c6bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up network training parameters\n",
    "x_range = [0, 2]\n",
    "hid_lay = [128, 128, 128]\n",
    "num_equations = 2\n",
    "iterations = 1\n",
    "\n",
    "# set-up initial conditions for various differential equations\n",
    "v_list_one_eq = [torch.tensor([[0.12]]), torch.tensor([[0.87]]), \n",
    "          torch.tensor([[0.34]]), torch.tensor([[0.75]])]\n",
    "\n",
    "v_list_two_eq = [torch.tensor([[0.12], [0.35]]), torch.tensor([[0.87], [0.62]]), \n",
    "          torch.tensor([[0.34], [0.69]]), torch.tensor([[0.75], [0.25]])]\n",
    "\n",
    "v_list_three_eq = [torch.tensor([[0.12], [0.35], [0.43]]), torch.tensor([[0.87], [0.62], [0.05]]), \n",
    "          torch.tensor([[0.34], [0.69], [0.32]]), torch.tensor([[0.75], [0.25], [0.11]])]\n",
    "\n",
    "# set up A matrix for various differential equations\n",
    "A_single_eq = [torch.tensor([[1.05]]), torch.tensor([[1.15]]), \n",
    "               torch.tensor([[0.55]]), torch.tensor([[0.25]])]\n",
    "\n",
    "A_non_coupled_2_eq_easy = [torch.tensor([[1.1, 0], [0, 1.1]]), torch.tensor([[0.19, 0], [0, 0.19]]),\n",
    "                           torch.tensor([[1.8, 0], [0, 1.8]]), torch.tensor([[0.75, 0], [0, 0.75]])]\n",
    "\n",
    "A_non_coupled_2_eq_hard = [torch.tensor([[1.1, 0], [0, 3]]), torch.tensor([[1.45, 0], [0, 1.24]]),\n",
    "                           torch.tensor([[0.13, 0], [0, 2.3]]), torch.tensor([[0.1, 0], [0, 2.1]])]\n",
    "\n",
    "A_coupled_2_eq_easy = [torch.tensor([[0, 1.1], [1.1, 0]]), torch.tensor([[0, 0.19], [0.19, 0]]), \n",
    "                       torch.tensor([[0, 1.8], [1.8, 0]]), torch.tensor([[0, 0.75], [0.75, 0]])]\n",
    "\n",
    "A_coupled_2_eq_hard = [torch.tensor([[0, 1.1], [1.5, 0]]), torch.tensor([[0, 0.01], [0.34, 0]]), \n",
    "                       torch.tensor([[0, 0.5], [0.9, 0]]), torch.tensor([[0, 0.32], [0.43, 0]])]\n",
    "\n",
    "# A_coupled_2_eq_hard = [torch.tensor([[0, 1.1], [1.5, 0]]), torch.tensor([[0, 1.1], [1.5, 0]]), \n",
    "#                        torch.tensor([[0, 1.1], [1.5, 0]]), torch.tensor([[0, 1.1], [1.5, 0]])]\n",
    "\n",
    "A_coupled_3_eq = [torch.tensor([[0, 0, 2.],[2., 0, 0],[0, 2., 0]]), torch.tensor([[0, 0, 0.56],[0.56, 0, 0],[0, 0.56, 0]]),\n",
    "                  torch.tensor([[0, 0, 1.12],[1.12, 0, 0],[0, 1.12, 0]]), torch.tensor([[0, 0, 1.73],[1.73, 0, 0],[0, 1.73, 0]])]\n",
    "\n",
    "# analytical solutions for various differential equations\n",
    "true_single_eq = [lambda x, v: (v[0] * np.exp(-1.05 * x)).numpy()]\n",
    "\n",
    "true_non_coupled_2_eq_easy = [lambda x, v: (v[0] * np.exp(-1.1 * x)).numpy(), \n",
    "                              lambda x, v: (v[1] * np.exp(-1.1 * x)).numpy()]\n",
    "\n",
    "true_non_coupled_2_eq_hard = [lambda x, v: (v[0] * np.exp(-1.1 * x)).numpy(), \n",
    "                              lambda x, v: (v[1] * np.exp(-3 * x)).numpy()]\n",
    "\n",
    "true_coupled_2_eq_easy = [lambda x, v: (0.5 * v[0] * np.exp(-1.1 * x) * (np.exp(2.2 * x) + 1) - (0.5 * v[1] * np.exp(-1.1 * x) * (np.exp(2.2 * x) - 1))).numpy(),\n",
    "                         lambda x, v: (0.5 * v[1] * np.exp(-1.1 * x) * (np.exp(2.2 * x) + 1) - (0.5 * v[0] * np.exp(-1.1 * x) * (np.exp(2.2 * x) - 1))).numpy()]\n",
    "\n",
    "true_coupled_2_eq_hard = [lambda x, v: (0.5 * v[0] * np.exp(-1.28452 * x) * (np.exp(2.56905 * x) + 1) - (0.428174 * v[1] * np.exp(-1.28452 * x) * (np.exp(2.56905 * x) - 1))).numpy(),\n",
    "                          lambda x, v: (0.5 * v[1] * np.exp(-1.28452 * x) * (np.exp(2.56905 * x) + 1) - (0.583874 * v[0] * np.exp(-1.28452 * x) * (np.exp(2.56905 * x) - 1))).numpy()]\n",
    "\n",
    "true_coupled_3_eq = [lambda x, v : (1/3 * v[0] * np.exp(-2 * x) * (2 * np.exp(3 * x) * np.cos(np.sqrt(3) * x) + 1) - 1/3 * v[1] * np.exp(-2 * x) * (np.exp(3 * x) * (np.cos(np.sqrt(3) * x) - np.sqrt(3) * np.sin(np.sqrt(3) * x)) - 1) - 1/3 * v[2] * np.exp(-2 * x) * (np.exp(3 * x) * (np.cos(np.sqrt(3) * x) + np.sqrt(3) * np.sin(np.sqrt(3) * x)) - 1)).numpy(), \n",
    "                     lambda x, v : (1/3 * v[1] * np.exp(-2 * x) * (2 * np.exp(3 * x) * np.cos(np.sqrt(3) * x) + 1) - 1/3 * v[0] * np.exp(-2 * x) * (np.exp(3 * x) * (np.cos(np.sqrt(3) * x) + np.sqrt(3) * np.sin(np.sqrt(3) * x)) - 1) - 1/3 * v[2] * np.exp(-2 * x) * (np.exp(3 * x) * (np.cos(np.sqrt(3) * x) - np.sqrt(3) * np.sin(np.sqrt(3) * x)) - 1)).numpy(), \n",
    "                     lambda x, v : (1/3 * v[2] * np.exp(-2 * x) * (2 * np.exp(3 * x) * np.cos(np.sqrt(3) * x) + 1) - 1/3 * v[0] * np.exp(-2 * x) * (np.exp(3 * x) * (np.cos(np.sqrt(3) * x) - np.sqrt(3) * np.sin(np.sqrt(3) * x)) - 1) - 1/3 * v[1] * np.exp(-2 * x) * (np.exp(3 * x) * (np.cos(np.sqrt(3) * x) + np.sqrt(3) * np.sin(np.sqrt(3) * x)) - 1)).numpy()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029620d0",
   "metadata": {},
   "source": [
    "### Step 2: Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "588949ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Iteration 1\n",
      "u shape:  torch.Size([500, 2, 1])\n",
      "A shape:  torch.Size([2, 2])\n",
      "A_times_u shape:  torch.Size([500, 2, 1])\n",
      "du_dt shape:  torch.Size([500, 2, 1])\n",
      "L_t_term shape:  torch.Size([500, 2, 1])\n",
      "L_t shape:  torch.Size([500, 1, 1])\n",
      "u_0 shape:  torch.Size([2, 1])\n",
      "v shape:  torch.Size([2, 1])\n",
      "u shape:  torch.Size([500, 2, 1])\n",
      "A shape:  torch.Size([2, 2])\n",
      "A_times_u shape:  torch.Size([500, 2, 1])\n",
      "du_dt shape:  torch.Size([500, 2, 1])\n",
      "L_t_term shape:  torch.Size([500, 2, 1])\n",
      "L_t shape:  torch.Size([500, 1, 1])\n",
      "u_0 shape:  torch.Size([2, 1])\n",
      "v shape:  torch.Size([2, 1])\n",
      "u shape:  torch.Size([500, 2, 1])\n",
      "A shape:  torch.Size([2, 2])\n",
      "A_times_u shape:  torch.Size([500, 2, 1])\n",
      "du_dt shape:  torch.Size([500, 2, 1])\n",
      "L_t_term shape:  torch.Size([500, 2, 1])\n",
      "L_t shape:  torch.Size([500, 1, 1])\n",
      "u_0 shape:  torch.Size([2, 1])\n",
      "v shape:  torch.Size([2, 1])\n",
      "u shape:  torch.Size([500, 2, 1])\n",
      "A shape:  torch.Size([2, 2])\n",
      "A_times_u shape:  torch.Size([500, 2, 1])\n",
      "du_dt shape:  torch.Size([500, 2, 1])\n",
      "L_t_term shape:  torch.Size([500, 2, 1])\n",
      "L_t shape:  torch.Size([500, 1, 1])\n",
      "u_0 shape:  torch.Size([2, 1])\n",
      "v shape:  torch.Size([2, 1])\n",
      "Model Training Complete in 0.060 seconds\n"
     ]
    }
   ],
   "source": [
    "# choose the equation(s) to work with\n",
    "A_list = A_coupled_2_eq_hard\n",
    "true_funct = true_coupled_2_eq_hard\n",
    "v_list = v_list_two_eq\n",
    "num_heads = 4\n",
    "head_to_track = 'head 2'\n",
    "\n",
    "# run model which has two non-coupled equations\n",
    "loss_hist, trained_model, model_time, MSE_hist = run_model(iterations=iterations, x_range=x_range, \n",
    "                                                           A_list=A_list, v_list=v_list, hid_lay=hid_lay,\n",
    "                                                           num_equations=num_equations, num_heads=num_heads,\n",
    "                                                           true_functs=true_funct,\n",
    "                                                           head_to_track=head_to_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d623a5",
   "metadata": {},
   "source": [
    "### Step 3: View Training Results and History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the total loss, network vs true solution and MSE for two non-coupled equations\n",
    "plot_loss_mse_and_solution(x_range=x_range, true_functs=true_funct, iterations=iterations, \n",
    "                           trained_model=trained_model, v_list=v_list,\n",
    "                           train_losses=loss_hist['Ltotal_losses'], \n",
    "                           loss_label='Total Loss ($L_D$)', mses=MSE_hist, \n",
    "                           head_to_track=head_to_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff1006",
   "metadata": {},
   "source": [
    "### Step 4a: Perform One-Shot Transfer Learning (use an initial condition that was trained on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, H_0, dH_dt_new, t_eval = compute_H_and_dH_dt(x_range[0], x_range[1], trained_model, num_equations, hid_lay)\n",
    "\n",
    "# pick an initial condition and matrix from those that were trained on (easy case)\n",
    "v = v_list[0]\n",
    "A = A_list[0]\n",
    "M_inv, W_out = analytically_compute_weights(dH_dt_new, H, H_0, t_eval, v, A)\n",
    "plot_transfer_learned_and_analytical(H, W_out, t_eval, v, num_equations, true_funct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02418b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING CELL: DELETE\n",
    "\n",
    "# generate a set of times to evaluate with\n",
    "rng = np.random.default_rng()\n",
    "t_eval = torch.arange(0., 2., 0.001, requires_grad=True)\n",
    "t_eval = t_eval[rng.integers(low=0, high=len(t_eval), size=500)]\n",
    "t_eval = t_eval.reshape(-1, 1)\n",
    "\n",
    "# forward pass with t_eval to extract H \n",
    "output, H = trained_model(t_eval)\n",
    "\n",
    "print(H.shape)\n",
    "\n",
    "# this will work instead of the jacobian...\n",
    "torch.hstack([torch.autograd.grad(H[:, i], t_eval, grad_outputs = torch.ones_like(H[:, i]), create_graph=True)[0] for i in range(H.shape[1])])\n",
    "\n",
    "\n",
    "# # reshape \"H\" to batch_size X num_equations X d // num_equations\n",
    "# H = H.reshape(-1, num_equations, hid_lay[-1] // num_equations)\n",
    "# H = torch.cat((torch.ones(len(t_eval), num_equations, 1), H), 2)\n",
    "\n",
    "# # forward pass with t = 0 to extract H_0\n",
    "# output_0, H_0 = trained_model(torch.tensor([[0.]], dtype=torch.float32, requires_grad=True))\n",
    "# # reshape \"H_0\" to batch_size X num_equations X d // num_equations\n",
    "# H_0 = H_0.reshape(-1, num_equations, hid_lay[-1] // num_equations)\n",
    "# H_0 = torch.cat((torch.ones(1, num_equations, 1), H_0), 2).squeeze()\n",
    "# H_0 = H_0.unsqueeze(dim=0) if num_equations == 1 else H_0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dH_dt (note: time intensive due to large jacobian calculation)\n",
    "# trained_model_H_only = lambda x: trained_model(x)[1]\n",
    "#     from neurodiffeq import unsafe_diff as diff\n",
    "#     H_dot = torch.hstack([diff(trained_model_H_only(t)[:, i:i+1], t) for i in range(trained_model_H_only(t).shape[1])])\n",
    "\n",
    "# dH_dt = torch.autograd.functional.jacobian(trained_model_H_only, t_eval, create_graph=False).sum(axis=2) \n",
    "dH_dt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292050b9",
   "metadata": {},
   "source": [
    "### Step 4b: Perform One-Shot Transfer Learning (use a new, unseen initial condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c81002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose new initial condition that has not been trained on (harder case)\n",
    "v_transfer = torch.tensor([[0.69], [0.05], [0.23]])\n",
    "# compute the transfer learned solution\n",
    "W_out_transfer = torch.matmul(torch.matmul(M_inv, H_0.T), v_transfer)\n",
    "plot_transfer_learned_and_analytical(H, W_out_transfer, t_eval, v_transfer, num_equations, true_funct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def4061",
   "metadata": {},
   "source": [
    "### Step 4c: Perform One-Shot Transfer Learning (use a new differential equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new differential equation to apply training to\n",
    "factor_1 = 3.53\n",
    "transfer_A = torch.tensor([[factor_1, 0], [0, factor_1]])\n",
    "\n",
    "M_inv_new, W_out_new_A = analytically_compute_weights(dH_dt_new, H, H_0, t_eval, v, transfer_A)\n",
    "\n",
    "true_non_coupled_2_eq_easy_transfer = [lambda x, v: (v[0] * np.exp(-factor_1 * x)).numpy(), \n",
    "                                     lambda x, v: (v[1] * np.exp(-factor_1 * x)).numpy()]\n",
    "\n",
    "# W_out_transfer = torch.matmul(torch.matmul(M_inv, H_0.T), v_transfer)\n",
    "plot_transfer_learned_and_analytical(H, W_out_new_A, t_eval, v, num_equations, true_non_coupled_2_eq_easy_transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c68c7",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the residuals \n",
    "\n",
    "# compute the residuals to determine effectiveness of transfer learning when changing initial condition\n",
    "residuals = ((torch.matmul(dH_dt_new, W_out_transfer) + torch.matmul(A, torch.matmul(H, W_out_transfer))).squeeze()) ** 2\n",
    "residuals = residuals.reshape(-1, 1).detach().numpy()\n",
    "\n",
    "print(f\"Mean of residuals: {residuals.mean()}\")\n",
    "initial_cond_residuals = ((torch.matmul(H_0, W_out_transfer) - v_transfer) ** 2).mean()\n",
    "print(f\"Initial Condition Residual: {initial_cond_residuals.item()}\")\n",
    "\n",
    "# plot the residuals\n",
    "plt.plot(residuals);\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Plot of Residuals\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time One-Shot Transfer Learning\n",
    "\n",
    "start_one_shot_time = time.time()\n",
    "\n",
    "# compute weights for 10000 different initial conditions\n",
    "for i in range(10000): \n",
    "    random_u_0 = torch.rand((num_equations, 1))\n",
    "    w_out = torch.matmul(torch.matmul(M_inv, H_0.T), random_u_0)\n",
    "    \n",
    "end_one_shot_time = time.time() \n",
    "total_one_shot = end_one_shot_time - start_one_shot_time\n",
    "print(f\"Total Time to evaluate w_out for 10000 different initial conditions: {total_one_shot: .2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e9caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "# put in an assert before training so that the reshape with H will work out...\n",
    "\n",
    "# latex the forcing term equation from whiteboard\n",
    "\n",
    "# code-up the new version of the equation with forcing function\n",
    "\n",
    "# remove jacobian call with something else \n",
    "\n",
    "# the true/analytical functions need to take in an \"A\" parameter so we can customize the plotting...\n",
    "    # need to use scipy library solve_ivp (or autodiff library and convert to lambda function)\n",
    "\n",
    "# try equations with non-square A\n",
    "\n",
    "# add in docstrings for all functions\n",
    "\n",
    "# write out math in notebook from Pythia and label it according to the right meeting week - DONE\n",
    "\n",
    "# write-out multi-headed approach in notebook - DONE\n",
    "\n",
    "# do github organize / update - DONE\n",
    "\n",
    "# modularize all code - DONE\n",
    "\n",
    "# try 3 coupled equations - DONE\n",
    "\n",
    "# time the one-shot transfer learning with different A - DONE\n",
    "\n",
    "# modularize all code including transfer learning so that it is part of main computation function, just in a conditional - DONE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
